{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Extended Experimental Results: Full Model Comparison & Confound Analysis\n\nThis notebook provides a comprehensive report of **all experimental conditions** evaluated during the development of the SpeechSense cognitive decline monitoring pipeline, covering:\n\n1. **DementiaNet dataset** (human-curated, 182 speakers, 354 clips)\n2. **LLM-assisted (agentic) dataset** (196 speakers, ~3,100 clips)\n3. **Holdout evaluation** (6 celebrity speakers, fully unseen)\n4. **Confound analysis** (metadata leakage, model-accessible artefacts, permutation tests)\n\nAll results use **speaker-grouped 5-fold cross-validation** (GroupKFold) unless stated otherwise.  \nThe primary metric is **AUC** (area under ROC curve). F1, accuracy, and precision are reported where available.\n\n---",
   "id": "47163462"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom IPython.display import display, Markdown, HTML\n\n# Paths relative to repository root (run notebook from repo root or adjust as needed)\nBASE = Path(\".\").resolve()\nif BASE.name == \"notebooks\":\n    BASE = BASE.parent\nREPRO = BASE / \"reproducibility\"\nMULTIMODAL = REPRO / \"multimodal\"\nHOLDOUT = BASE / \"dataset\" / \"holdout\"\n\ndef load_json(path):\n    with open(path) as f:\n        return json.load(f)\n\nprint(\"Paths configured.\")",
   "id": "2cd351d2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. DementiaNet Dataset (Human-Curated)\n",
    "\n",
    "**Dataset**: 182 speakers (84 dementia, 98 control), 354 pre-symptoms video clips curated by domain experts.  \n",
    "**Audio source**: YouTube interviews, Pyannote-transcribed.  \n",
    "**Evaluation**: 5-fold speaker-grouped cross-validation.\n",
    "\n",
    "### 1.1 Summary of All DementiaNet Conditions"
   ],
   "id": "b23e7f76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── DementiaNet: all conditions summary table ──\n",
    "\n",
    "dnet_results = [\n",
    "    {\n",
    "        \"Model\": \"HeAR frozen embeddings\",\n",
    "        \"Classifier\": \"SVM-RBF\",\n",
    "        \"AUC\": 0.748,\n",
    "        \"Std\": 0.078,\n",
    "        \"Acc\": 0.681,\n",
    "        \"F1\": 0.632,\n",
    "        \"Notes\": \"Google HeAR CLS attention features, speaker-level [mean,std]\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"HeAR frozen embeddings\",\n",
    "        \"Classifier\": \"LogReg PCA-64\",\n",
    "        \"AUC\": 0.722,\n",
    "        \"Std\": None,\n",
    "        \"Acc\": 0.681,\n",
    "        \"F1\": 0.623,\n",
    "        \"Notes\": \"From extended_condition_metrics (n=182 speakers)\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MiniRocket on mel spectrograms\",\n",
    "        \"Classifier\": \"LogReg PCA-128\",\n",
    "        \"AUC\": 0.770,\n",
    "        \"Std\": 0.056,\n",
    "        \"Acc\": 0.731,\n",
    "        \"F1\": 0.680,\n",
    "        \"Notes\": \"Model-free control; 2000 random convolutional kernels on 128-bin mel specs\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma text embeddings (Pyannote transcripts)\",\n",
    "        \"Classifier\": \"GBM\",\n",
    "        \"AUC\": 0.794,\n",
    "        \"Std\": 0.067,\n",
    "        \"Acc\": 0.706,\n",
    "        \"F1\": 0.683,\n",
    "        \"Notes\": \"Best text-only result (Speaker Full, GBM); n=177 speakers\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma text embeddings (Pyannote transcripts)\",\n",
    "        \"Classifier\": \"LogReg PCA-64\",\n",
    "        \"AUC\": 0.792,\n",
    "        \"Std\": 0.068,\n",
    "        \"Acc\": 0.730,\n",
    "        \"F1\": None,\n",
    "        \"Notes\": \"Alternative classifier setting\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma vision encoder (spectrograms, image-only)\",\n",
    "        \"Classifier\": \"LogReg PCA-64\",\n",
    "        \"AUC\": 0.730,\n",
    "        \"Std\": 0.044,\n",
    "        \"Acc\": 0.653,\n",
    "        \"F1\": 0.609,\n",
    "        \"Notes\": \"384x384 magma-colourmap mel spectrograms through MedGemma vision encoder\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma image+text multimodal\",\n",
    "        \"Classifier\": \"GBM\",\n",
    "        \"AUC\": 0.788,\n",
    "        \"Std\": 0.068,\n",
    "        \"Acc\": 0.742,\n",
    "        \"F1\": 0.700,\n",
    "        \"Notes\": \"Concatenated text + image embeddings, Speaker Full\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Late fusion (HeAR + MedGemma text stacking)\",\n",
    "        \"Classifier\": \"Stacking ensemble\",\n",
    "        \"AUC\": 0.793,\n",
    "        \"Std\": 0.082,\n",
    "        \"Acc\": 0.739,\n",
    "        \"F1\": 0.701,\n",
    "        \"Notes\": \"Nested 5-fold CV; meta-learner over 6 base models (176 matched speakers)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_dnet = pd.DataFrame(dnet_results)\n",
    "print(\"DementiaNet Dataset — All Conditions\")\n",
    "print(\"=\" * 80)\n",
    "display(df_dnet[[\"Model\", \"Classifier\", \"AUC\", \"Std\", \"Acc\", \"F1\"]].to_string(index=False))"
   ],
   "id": "044bb522"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 HeAR Frozen Embeddings — Detailed Results\n",
    "\n",
    "Google's **Health Acoustic Representations (HeAR)** model produces 2304-dim CLS attention features per audio clip.  \n",
    "Speaker-level aggregation: `[mean(2304), std(2304)]` = 4608 dims."
   ],
   "id": "d3fca463"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HeAR results from level_one_test/outputs/results_summary.json\n",
    "hear_results = load_json(ROCKET_RESULTS / \"level_one_test\" / \"outputs\" / \"results_summary.json\")\n",
    "\n",
    "print(\"HeAR Frozen Embeddings — DementiaNet (182 speakers, 353 clips)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: {hear_results['dataset']['total_speakers']} speakers, \"\n",
    "      f\"{hear_results['dataset']['total_files']} files, \"\n",
    "      f\"{hear_results['dataset']['total_windows']} 5s windows\")\n",
    "\n",
    "print(\"\\nSpeaker-level results:\")\n",
    "for clf, m in hear_results['speaker_level_results'].items():\n",
    "    print(f\"  {clf:10s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}  \"\n",
    "          f\"Acc = {m['mean_acc']:.3f}  F1 = {m['mean_f1']:.3f}\")\n",
    "\n",
    "print(\"\\nClip-level results (for reference):\")\n",
    "for clf, m in hear_results['clip_level_results'].items():\n",
    "    print(f\"  {clf:10s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}\")"
   ],
   "id": "ec4be312"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 MiniRocket on Mel Spectrograms — Model-Free Control\n",
    "\n",
    "**MiniRocket** (Dempster et al., 2021) applies 2000 random convolutional kernels to mel-spectrogram time series.  \n",
    "This is a **model-free baseline** — it uses no learned representations, only random convolutions on acoustic data.  \n",
    "It sets a performance floor: any model must beat MiniRocket to justify the complexity of learned embeddings."
   ],
   "id": "f77928da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniRocket results from level_one_test/outputs_rocket/rocket_summary.json\n",
    "rocket = load_json(ROCKET_RESULTS / \"level_one_test\" / \"outputs_rocket\" / \"rocket_summary.json\")\n",
    "\n",
    "print(\"MiniRocket on Mel Spectrograms — DementiaNet\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {rocket['n_speakers']} speakers, {rocket['n_clips']} clips\")\n",
    "print(f\"Features per clip: {rocket['rocket_features_per_clip']:,} (2000 kernels x ~124 output channels)\")\n",
    "\n",
    "print(\"\\nSpeaker-level results:\")\n",
    "for key in sorted(rocket.keys()):\n",
    "    if key.startswith(\"Speaker\"):\n",
    "        m = rocket[key]\n",
    "        print(f\"  {key:28s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}  \"\n",
    "              f\"Acc = {m['mean_acc']:.3f}  F1 = {m['mean_f1']:.3f}\")\n",
    "\n",
    "print(\"\\nClip-level results (for reference):\")\n",
    "for key in sorted(rocket.keys()):\n",
    "    if key.startswith(\"Clip\"):\n",
    "        m = rocket[key]\n",
    "        print(f\"  {key:28s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}\")"
   ],
   "id": "dd14fbab"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.4 ROCKET Frequency Band Ablation (Agentic Dataset)\n\nTo investigate which frequency ranges carry diagnostic information, MiniRocket was run on **sliced mel spectrograms**:\n\n| Band | Mel bins | Approx. frequency | Acoustic interpretation |\n|------|----------|-------------------|------------------------|\n| Full | 0–127 | 0–8 kHz | All acoustic information |\n| Bottom | 0–42 | 0–1 kHz | Prosody, fundamental frequency (F0), speech rhythm |\n| Middle | 43–85 | 1–4 kHz | Formant region, vowel/consonant articulation |\n| Top | 86–127 | 4+ kHz | Fricatives, noise, recording quality artefacts |\n\n**Key finding**: Signal is concentrated in the **bottom and middle bands** (prosodic and articulatory), not the top band — ruling out recording-quality artefacts as the primary driver.",
   "id": "a9eab902"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROCKET frequency band ablation results\n",
    "ablation = pd.read_csv(ROCKET_RESULTS / \"rocket_freq_ablation_results\" / \"ablation_summary.csv\")\n",
    "\n",
    "print(\"ROCKET Frequency Band Ablation — Agentic Dataset (dedup)\")\n",
    "print(\"=\" * 70)\n",
    "print(ablation.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Bottom (prosody):       AUC = 0.718 — speech rhythm and pitch carry signal\")\n",
    "print(\"  Middle (articulation):  AUC = 0.724 — formant structure carries signal\")\n",
    "print(\"  Top (fricatives/noise): AUC = 0.678 — weakest band, below full-spectrum\")\n",
    "print(\"  Full (all bands):       AUC = 0.705\")\n",
    "print(\"\\n  => Diagnostic signal resides in prosodic + articulatory bands,\")\n",
    "print(\"     not high-frequency recording artefacts.\")"
   ],
   "id": "a56d35bd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 MedGemma Text Embeddings — DementiaNet\n",
    "\n",
    "**Pipeline**: Pyannote cloud API transcription → MedGemma 4B-IT text-only (mean-pooled last hidden state) → 2560-dim per clip → speaker-level `[mean, std]` = 5120 dims → classifier."
   ],
   "id": "fd478f24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MedGemma text-only for DementiaNet\n",
    "dnet_text = load_json(MEDASR / \"outputs_medgemma_emb_dementianet_pyannote_p2\" / \"text_only_metrics_dementianet_human.json\")\n",
    "\n",
    "print(\"MedGemma Text Embeddings (Pyannote Transcripts) — DementiaNet\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for key in sorted(dnet_text.keys()):\n",
    "    if key.startswith(\"Speaker\"):\n",
    "        m = dnet_text[key]\n",
    "        print(f\"  {key:28s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}  \"\n",
    "              f\"Acc = {m['mean_acc']:.3f}\")"
   ],
   "id": "f9de17b4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 MedGemma Vision & Multimodal — DementiaNet\n",
    "\n",
    "Mel spectrograms rendered as 384x384 PIL images (magma colourmap) passed through MedGemma's vision encoder."
   ],
   "id": "407cbe62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DementiaNet multimodal summary\n",
    "dnet_mm = load_json(MULTIMODAL / \"outputs_multimodal_dementianet\" / \"multimodal_summary_dementianet.json\")\n",
    "\n",
    "print(\"MedGemma Multimodal — DementiaNet\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for modality in ['text_only', 'image_only', 'image_text']:\n",
    "    print(f\"\\n  {modality.upper()}:\")\n",
    "    for key in sorted(dnet_mm.keys()):\n",
    "        if key.startswith(modality):\n",
    "            m = dnet_mm[key]\n",
    "            setting = key.replace(f\"{modality} \", \"\")\n",
    "            print(f\"    {setting:28s}: AUC = {m['mean_auc']:.4f} +/- {m['std_auc']:.4f}\")"
   ],
   "id": "af8fc4e6"
  },
  {
   "cell_type": "markdown",
   "source": "### 1.7 DementiaNet: Pre-Symptoms Only Re-Run\n\nAll DementiaNet experiments above used the full dataset (including 27 after-symptoms clips from 10 speakers).  \nBelow, **every condition is re-run** excluding `after_symptoms` clips to check whether results hold on purely pre-symptomatic data.\n\n**After filtering**: 167 speakers, 314–315 clips (depending on match rates per modality).",
   "metadata": {},
   "id": "6f03b7aa"
  },
  {
   "cell_type": "code",
   "source": "# Load DementiaNet pre-symptoms re-run results\nNOTEBOOKS = BASE / \"gh_hackathon\" / \"notebooks\"\ndnet_pre = load_json(NOTEBOOKS / \"dementianet_pre_symptoms_results.json\")\n\nprint(\"DementiaNet: All Data vs Pre-Symptoms Only\")\nprint(\"=\" * 100)\nprint()\n\n# ── MedGemma Text ──\nprint(\"MedGemma Text Embeddings (Pyannote transcripts)\")\nprint(\"-\" * 80)\nprint(f\"  {'Setting':<35s} {'All Data AUC':>13s} {'Pre-Symp AUC':>14s} {'Delta':>8s}\")\n\nmg_pre = dnet_pre[\"medgemma_text_pre_symptoms\"]\nall_data_text_refs = {\n    \"Speaker Full__LogReg\": 0.7920,\n    \"Speaker Full__GBM\": 0.7940,\n    \"Speaker PCA-64__LogReg\": 0.7830,\n    \"Speaker PCA-64__GBM\": 0.7710,\n    \"Speaker PCA-128__LogReg\": 0.7870,\n    \"Speaker PCA-128__GBM\": 0.7650,\n}\nfor key in sorted(mg_pre.keys(), key=lambda k: -mg_pre[k][\"mean_auc\"]):\n    pre_auc = mg_pre[key][\"mean_auc\"]\n    all_auc = all_data_text_refs.get(key)\n    if all_auc is not None:\n        delta = pre_auc - all_auc\n        print(f\"  {key:<35s} {all_auc:13.4f} {pre_auc:14.4f} {delta:+8.4f}\")\n    else:\n        print(f\"  {key:<35s} {'—':>13s} {pre_auc:14.4f}\")\n\n# ── HeAR ──\nprint(f\"\\nHeAR Frozen Embeddings (CLS attention)\")\nprint(\"-\" * 80)\nprint(f\"  {'Setting':<35s} {'All Data AUC':>13s} {'Pre-Symp AUC':>14s} {'Delta':>8s}\")\n\nhear_pre = dnet_pre[\"hear_pre_symptoms\"]\nall_data_hear_refs = {\n    \"Speaker Full__SVM_RBF\": 0.7480,\n    \"Speaker Full__LogReg\": 0.7290,\n    \"Speaker Full__GBM\": 0.7340,\n    \"Speaker Full__MLP\": 0.7210,\n}\nfor key in sorted(hear_pre.keys(), key=lambda k: -hear_pre[k].get(\"mean_auc\", 0)):\n    if \"Speaker\" not in key:\n        continue\n    pre_auc = hear_pre[key][\"mean_auc\"]\n    all_auc = all_data_hear_refs.get(key)\n    if all_auc is not None:\n        delta = pre_auc - all_auc\n        print(f\"  {key:<35s} {all_auc:13.4f} {pre_auc:14.4f} {delta:+8.4f}\")\n    else:\n        print(f\"  {key:<35s} {'—':>13s} {pre_auc:14.4f}\")\n\n# ── MiniRocket ──\nprint(f\"\\nMiniRocket on Mel Spectrograms\")\nprint(\"-\" * 80)\nprint(f\"  {'Setting':<35s} {'All Data AUC':>13s} {'Pre-Symp AUC':>14s} {'Delta':>8s}\")\n\nrk_pre = dnet_pre[\"rocket_pre_symptoms\"]\nall_data_rk_refs = {\n    \"Speaker PCA-128__LogReg\": 0.7700,\n    \"Speaker PCA-128__GBM\": 0.7230,\n    \"Speaker PCA-64__LogReg\": 0.7590,\n    \"Speaker PCA-64__GBM\": 0.7350,\n}\nfor key in sorted(rk_pre.keys(), key=lambda k: -rk_pre[k].get(\"mean_auc\", 0)):\n    if \"Speaker\" not in key:\n        continue\n    pre_auc = rk_pre[key][\"mean_auc\"]\n    all_auc = all_data_rk_refs.get(key)\n    if all_auc is not None:\n        delta = pre_auc - all_auc\n        print(f\"  {key:<35s} {all_auc:13.4f} {pre_auc:14.4f} {delta:+8.4f}\")\n    else:\n        print(f\"  {key:<35s} {'—':>13s} {pre_auc:14.4f}\")\n\n# ── Multimodal ──\nprint(f\"\\nMedGemma Multimodal (text/image/image+text)\")\nprint(\"-\" * 80)\nprint(f\"  {'Setting':<45s} {'All Data AUC':>13s} {'Pre-Symp AUC':>14s} {'Delta':>8s}\")\n\nmm_pre = dnet_pre[\"multimodal_pre_symptoms\"]\nall_data_mm_refs = {\n    \"text_only\": {\"Speaker Full__GBM\": 0.7900, \"Speaker Full__LogReg\": 0.7850},\n    \"image_only\": {\"Speaker Full__GBM\": 0.7300, \"Speaker PCA-64__LogReg\": 0.7300},\n    \"image_text\": {\"Speaker Full__GBM\": 0.7880, \"Speaker Full__LogReg\": 0.7830},\n}\nfor cond in [\"text_only\", \"image_only\", \"image_text\"]:\n    if cond not in mm_pre:\n        continue\n    print(f\"\\n  {cond.upper()}:\")\n    cond_pre = mm_pre[cond]\n    refs = all_data_mm_refs.get(cond, {})\n    for key in sorted(cond_pre.keys(), key=lambda k: -cond_pre[k][\"mean_auc\"]):\n        pre_auc = cond_pre[key][\"mean_auc\"]\n        all_auc = refs.get(key)\n        if all_auc is not None:\n            delta = pre_auc - all_auc\n            print(f\"    {key:<43s} {all_auc:13.4f} {pre_auc:14.4f} {delta:+8.4f}\")\n        else:\n            print(f\"    {key:<43s} {'—':>13s} {pre_auc:14.4f}\")\n\n# ── Summary ──\nprint(f\"\\n\\nSUMMARY: Best Result Per Model (All Data vs Pre-Symptoms)\")\nprint(\"=\" * 80)\nprint(f\"  {'Model':<40s} {'All Data':>10s} {'Pre-Symp':>10s} {'Delta':>8s}\")\nprint(\"-\" * 70)\n\nsummary_pairs = [\n    (\"MedGemma text (Full LogReg)\", 0.7940, mg_pre[\"Speaker Full__LogReg\"][\"mean_auc\"]),\n    (\"HeAR (Full SVM-RBF)\", 0.7480, hear_pre[\"Speaker Full__SVM_RBF\"][\"mean_auc\"]),\n    (\"MiniRocket (PCA-128 LogReg)\", 0.7700, rk_pre[\"Speaker PCA-128__LogReg\"][\"mean_auc\"]),\n    (\"Multimodal text_only (Full GBM)\", 0.7900, mm_pre[\"text_only\"][\"Speaker Full__GBM\"][\"mean_auc\"]),\n    (\"Multimodal image_only (Full GBM)\", 0.7300, mm_pre[\"image_only\"][\"Speaker Full__GBM\"][\"mean_auc\"]),\n    (\"Multimodal image_text (Full GBM)\", 0.7880, mm_pre[\"image_text\"][\"Speaker Full__GBM\"][\"mean_auc\"]),\n]\n\nfor name, all_auc, pre_auc in summary_pairs:\n    delta = pre_auc - all_auc\n    print(f\"  {name:<40s} {all_auc:10.4f} {pre_auc:10.4f} {delta:+8.4f}\")\n\nprint(\"\\nObservation:\")\nprint(\"  Most models IMPROVE when after-symptoms clips are excluded.\")\nprint(\"  This suggests the 27 after-symptoms clips (from 10 speakers) added noise\")\nprint(\"  rather than meaningful signal — the pre-symptomatic data alone is more consistent.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "2a7a63ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LLM-Assisted (Agentic) Dataset\n",
    "\n",
    "**Dataset**: 196 speakers (92 dementia, 104 control), ~3,100 clips sourced via an LLM-assisted speaker-selection pipeline.  \n",
    "After excluding post-symptom clips: **188 speakers, 2,571 clips** (our primary evaluation set).\n",
    "\n",
    "### 2.1 Summary of All Agentic Conditions"
   ],
   "id": "bb5ebcb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_results = [\n",
    "    {\n",
    "        \"Model\": \"MedASR → MedGemma text (original pipeline)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.619,\n",
    "        \"F1\": None,\n",
    "        \"Acc\": None,\n",
    "        \"Notes\": \"20s-chunked transcripts via MedASR; chunking degrades performance\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Transcript source ablation (manifest transcripts)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.895,\n",
    "        \"F1\": 0.813,\n",
    "        \"Acc\": 0.827,\n",
    "        \"Notes\": \"Same pipeline but using pre-existing manifest transcripts instead of MedASR\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma text-only (final, manifest text)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.904,\n",
    "        \"F1\": 0.802,\n",
    "        \"Acc\": 0.811,\n",
    "        \"Notes\": \"Speaker Full LogReg; multimodal pipeline text-only branch\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma vision (spectrograms, image-only)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.636,\n",
    "        \"F1\": 0.491,\n",
    "        \"Acc\": 0.556,\n",
    "        \"Notes\": \"384x384 magma mel spectrograms; near-chance performance\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MedGemma image+text multimodal\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.840,\n",
    "        \"F1\": 0.762,\n",
    "        \"Acc\": 0.781,\n",
    "        \"Notes\": \"Image+text concat; WORSE than text-only (image adds noise)\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"HeAR CLS attention features (standalone)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.726,\n",
    "        \"F1\": 0.663,\n",
    "        \"Acc\": 0.668,\n",
    "        \"Notes\": \"Speaker Full LogReg; moderate but below text-only\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Late fusion (text + HeAR, learned alpha)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.913,\n",
    "        \"F1\": 0.819,\n",
    "        \"Acc\": 0.827,\n",
    "        \"Notes\": \"Marginal gain over text-only; alpha ~ 0.02-0.15\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Feature concatenation (text + HeAR)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.917,\n",
    "        \"F1\": 0.862,\n",
    "        \"Acc\": 0.878,\n",
    "        \"Notes\": \"9728-dim concat; HIGHEST CV AUC but FAILED on holdout (0.667)\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Acoustic narrative fusion (text+acoustic)\",\n",
    "        \"Data\": \"all\",\n",
    "        \"AUC\": 0.921,\n",
    "        \"F1\": 0.809,\n",
    "        \"Acc\": 0.816,\n",
    "        \"Notes\": \"Text prompt enriched with 14 librosa-derived acoustic metrics\",\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Acoustic narrative (pre-symptoms only) ★\",\n",
    "        \"Data\": \"pre-symp\",\n",
    "        \"AUC\": 0.911,\n",
    "        \"F1\": 0.829,\n",
    "        \"Acc\": 0.851,\n",
    "        \"Notes\": \"FINAL MODEL: 188 speakers, 2571 clips; deployed in app\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_ag = pd.DataFrame(agentic_results)\n",
    "print(\"LLM-Assisted (Agentic) Dataset — All Conditions\")\n",
    "print(\"=\" * 90)\n",
    "print(df_ag[[\"Model\", \"Data\", \"AUC\", \"F1\", \"Acc\"]].to_string(index=False))"
   ],
   "id": "a3bd9546"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pre-Symptoms Model — Detailed CV Metrics\n",
    "\n",
    "The **final deployed model** uses text + acoustic narrative embeddings, pre-symptoms clips only."
   ],
   "id": "61eda623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-symptoms model detailed metrics\n",
    "pre_symp_summary = load_json(\n",
    "    MULTIMODAL / \"outputs_multimodal_agentic_manifest_text_acoustic_narrative_plus_hear_no_after_symptoms\"\n",
    "    / \"summary_no_after_symptoms.json\"\n",
    ")\n",
    "pre_symp_clf = load_json(\n",
    "    MULTIMODAL / \"outputs_multimodal_agentic_manifest_text_acoustic_narrative_plus_hear_no_after_symptoms\"\n",
    "    / \"classification_metrics_f1_accuracy_no_after_symptoms.json\"\n",
    ")\n",
    "\n",
    "print(\"Pre-Symptoms Model (text + acoustic narrative) — Detailed\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Speakers: {pre_symp_clf['n_speakers']}\")\n",
    "print(f\"Clips: {pre_symp_clf['n_common_clips']} (excluded after-symptoms: {pre_symp_clf['n_excluded_after_symptoms']})\")\n",
    "print(f\"Feature dim: {pre_symp_summary['dims']['tan_speaker_dim']} (MedGemma [mean,std])\")\n",
    "\n",
    "tan = pre_symp_clf['models_full_dim']['text_plus_acoustic_narrative']\n",
    "print(f\"\\nText + Acoustic Narrative (Full LogReg):\")\n",
    "print(f\"  AUC:      {tan['auc']:.4f}\")\n",
    "print(f\"  F1:       {tan['f1']:.4f}\")\n",
    "print(f\"  Accuracy: {tan['accuracy']:.4f}\")\n",
    "\n",
    "baseline = pre_symp_summary['baselines']['text_plus_acoustic_narrative Speaker Full__LogReg']\n",
    "print(f\"\\n  Per-fold AUCs: {[f'{a:.4f}' for a in baseline['fold_aucs']]}\")\n",
    "print(f\"  Mean AUC: {baseline['mean_auc']:.4f} +/- {baseline['std_auc']:.4f}\")"
   ],
   "id": "c58edbb0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 HeAR Fusion Variants (All Data vs Pre-Symptoms)\n",
    "\n",
    "Three fusion strategies were tested with HeAR CLS attention features:"
   ],
   "id": "f7e9bc78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-data fusion metrics\n",
    "all_data_clf = load_json(\n",
    "    MULTIMODAL / \"outputs_multimodal_agentic_manifest_text_acoustic_narrative_plus_hear\"\n",
    "    / \"classification_metrics_f1_accuracy.json\"\n",
    ")\n",
    "\n",
    "print(\"HeAR Fusion Variants\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# All-data\n",
    "print(f\"\\nALL DATA (196 speakers):\")\n",
    "print(f\"{'Model':<45s} {'AUC':>8s} {'F1':>8s} {'Acc':>8s}\")\n",
    "print(\"-\" * 69)\n",
    "for name, m in all_data_clf['models_full_dim'].items():\n",
    "    label = name.replace('_', ' ').title()\n",
    "    print(f\"  {label:<43s} {m['auc']:8.4f} {m['f1']:8.4f} {m['accuracy']:8.4f}\")\n",
    "\n",
    "hear_all = all_data_clf['late_fusion_details']['hear_metrics']\n",
    "print(f\"  {'HeAR standalone':<43s} {hear_all['auc']:8.4f} {hear_all['f1']:8.4f} {hear_all['accuracy']:8.4f}\")\n",
    "\n",
    "# Pre-symptoms\n",
    "print(f\"\\nPRE-SYMPTOMS ONLY (188 speakers):\")\n",
    "print(f\"{'Model':<45s} {'AUC':>8s} {'F1':>8s} {'Acc':>8s}\")\n",
    "print(\"-\" * 69)\n",
    "for name, m in pre_symp_clf['models_full_dim'].items():\n",
    "    label = name.replace('_', ' ').title()\n",
    "    print(f\"  {label:<43s} {m['auc']:8.4f} {m['f1']:8.4f} {m['accuracy']:8.4f}\")\n",
    "\n",
    "hear_pre = pre_symp_clf['late_fusion_details']['hear_metrics']\n",
    "print(f\"  {'HeAR standalone':<43s} {hear_pre['auc']:8.4f} {hear_pre['f1']:8.4f} {hear_pre['accuracy']:8.4f}\")\n",
    "\n",
    "print(\"\\n\\nKey observation: Feature concatenation gives highest CV AUC (0.917/0.907)\")\n",
    "print(\"but FAILED on holdout evaluation (see Section 3). HeAR was subsequently dropped.\")"
   ],
   "id": "efda303a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extended Condition Metrics (All Conditions, Best Settings)\n",
    "\n",
    "Comprehensive comparison including chunked transcripts, manifest text, and DementiaNet."
   ],
   "id": "051f2c13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended condition metrics from docs/figures\n",
    "ext_metrics = load_json(DOCS / \"extended_condition_metrics.json\")\n",
    "\n",
    "print(\"All Conditions — Best Setting Per Condition\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Condition':<50s} {'Setting':<28s} {'AUC':>7s} {'F1':>7s} {'Acc':>7s} {'N':>5s}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for entry in ext_metrics:\n",
    "    print(f\"  {entry['condition']:<48s} {entry['setting']:<28s} \"\n",
    "          f\"{entry['AUC']:7.4f} {entry['F1']:7.4f} {entry['Accuracy']:7.4f} {entry['n_samples']:5d}\")"
   ],
   "id": "58c1d79e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Holdout Evaluation (Unseen Speakers)\n\n**6 speakers** were held out entirely from training — their interviews were sourced, processed, and scored using the trained models **without any retraining or fine-tuning**.\n\n| Speaker | Group | Clips |\n|---------|-------|-------|\n| HOLDOUT_001 | Dementia | 9 |\n| HOLDOUT_002 | Dementia | 16 |\n| HOLDOUT_003 | Dementia | 11 |\n| HOLDOUT_004 | Control | 17 |\n| HOLDOUT_005 | Control | 11 |\n| HOLDOUT_006 | Control | 21 |\n\n*Two additional speakers (HOLDOUT_007, HOLDOUT_008) were originally processed but excluded from final analysis — their clips were force-analysed by overriding quality filters.*\n\n### 3.1 HeAR-Based Models on Holdout (Failed)",
   "id": "76c0a5e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import roc_auc_score\n\n# Two speakers excluded — force-analysed by overriding quality filters\nEXCLUDED_HOLDOUT = {\"Rita Moreno\", \"Willie Nelson\"}\n\n# Anonymisation mapping for holdout speakers\nANON_MAP = {\n    \"Bruce Willis\": \"HOLDOUT_001\", \"Gene Wilder\": \"HOLDOUT_002\",\n    \"Tippi Hedren\": \"HOLDOUT_003\", \"Carol Burnett\": \"HOLDOUT_004\",\n    \"Jane Goodall\": \"HOLDOUT_005\", \"Michael Caine\": \"HOLDOUT_006\",\n    \"Rita Moreno\": \"HOLDOUT_007\", \"Willie Nelson\": \"HOLDOUT_008\",\n}\ndef anon(name):\n    return ANON_MAP.get(name, name)\n\ndef filter_holdout_model(model_dict):\n    \"\"\"Filter excluded speakers and recalculate metrics.\"\"\"\n    speakers = [s for s in model_dict['speakers'] if s['speaker_name'] not in EXCLUDED_HOLDOUT]\n    y_true = [1 if s['group'] == 'dementia' else 0 for s in speakers]\n    y_prob = [s['prob'] for s in speakers]\n    n_correct = sum(1 for s in speakers if s['correct'])\n    auc = roc_auc_score(y_true, y_prob) if len(set(y_true)) > 1 else float('nan')\n    acc = n_correct / len(speakers)\n    return {\n        'speakers': speakers, 'auc': auc, 'accuracy': acc,\n        'n_correct': n_correct, 'n_speakers': len(speakers),\n    }\n\n# Holdout evaluation results — HeAR-based models\nholdout_hear = load_json(HOLDOUT / \"holdout_evaluation_results.json\")\n\nprint(\"Holdout Evaluation — Feature Concatenation (text + HeAR)\")\nprint(\"=\" * 80)\n\nfor model_name, m in holdout_hear['models'].items():\n    fm = filter_holdout_model(m)\n    label = model_name.replace('_', ' ').title()\n    print(f\"\\n{label}:\")\n    print(f\"  AUC: {fm['auc']:.4f}  Accuracy: {fm['accuracy']:.3f}  Correct: {fm['n_correct']}/{fm['n_speakers']}\")\n    print(f\"  {'Speaker':<20s} {'Group':<10s} {'Prob':>7s} {'Pred':>6s} {'Correct':>8s}\")\n    print(\"  \" + \"-\" * 55)\n    for s in fm['speakers']:\n        correct = 'YES' if s['correct'] else 'NO'\n        print(f\"  {anon(s['speaker_name']):<20s} {s['group']:<10s} {s['prob']:7.4f} {s['pred']:6d} {correct:>8s}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"CRITICAL FINDING: Feature concatenation (text+HeAR) fails catastrophically on holdout.\")\nprint(\"HeAR CLS attention features encode VOICE FINGERPRINTS, not cognitive markers.\")",
   "id": "26858b49"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text-Only and Text + Acoustic Narrative on Holdout"
   ],
   "id": "3b422c1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Text-only holdout\nholdout_text = load_json(HOLDOUT / \"holdout_text_only_results.json\")\n\n# Text + acoustic narrative holdout\nholdout_tan = load_json(HOLDOUT / \"holdout_text_acoustic_narrative_results.json\")\n\nprint(\"Holdout Evaluation — Text-Based Models (no HeAR)\")\nprint(\"=\" * 90)\n\nmodels_to_show = [\n    (\"Text-only (all data model)\", filter_holdout_model(holdout_text['results'])),\n    (\"Text+acoustic narrative (all data)\", filter_holdout_model(holdout_tan['models']['text_acoustic_narrative'])),\n    (\"Text+acoustic narrative (pre-symptoms) ★\", filter_holdout_model(holdout_tan['models']['text_acoustic_narrative_no_after_symptoms'])),\n]\n\nprint(f\"\\n{'Model':<50s} {'AUC':>7s} {'Acc':>7s} {'Correct':>8s}\")\nprint(\"-\" * 75)\nfor name, fm in models_to_show:\n    print(f\"  {name:<48s} {fm['auc']:7.4f} {fm['accuracy']:7.3f} {fm['n_correct']}/{fm['n_speakers']}\")\n\nprint(\"\\nPer-speaker scores (pre-symptoms model — final deployed model):\")\nprint(f\"  {'Speaker':<20s} {'Group':<10s} {'Prob':>7s} {'Pred':>6s} {'Correct':>8s}\")\nprint(\"  \" + \"-\" * 55)\n\nbest_fm = filter_holdout_model(holdout_tan['models']['text_acoustic_narrative_no_after_symptoms'])\nfor s in best_fm['speakers']:\n    correct = 'YES' if s['correct'] else 'NO'\n    print(f\"  {anon(s['speaker_name']):<20s} {s['group']:<10s} {s['prob']:7.4f} {s['pred']:6d} {correct:>8s}\")\n\n# Dynamic observations\ndem_speakers = [s for s in best_fm['speakers'] if s['group'] == 'dementia']\ncon_speakers = [s for s in best_fm['speakers'] if s['group'] == 'control']\ndem_correct = sum(1 for s in dem_speakers if s['correct'])\ncon_correct = sum(1 for s in con_speakers if s['correct'])\n\nprint(f\"\\nKey observations:\")\nprint(f\"  - {dem_correct}/3 dementia speakers correctly identified:\")\nfor s in sorted(dem_speakers, key=lambda x: -x['prob']):\n    print(f\"      {anon(s['speaker_name'])}: prob {s['prob']:.4f}\")\nprint(f\"  - {con_correct}/3 control speakers correctly classified:\")\nfor s in sorted(con_speakers, key=lambda x: x['prob']):\n    tag = \"correct\" if s['correct'] else \"FALSE POSITIVE\"\n    print(f\"      {anon(s['speaker_name'])}: prob {s['prob']:.4f} ({tag})\")",
   "id": "03684de9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Why HeAR Failed: Voice Fingerprinting Analysis\n\nThe HeAR model's CLS attention features act as **voice fingerprints** — they encode speaker identity rather than cognitive markers.\n\n**Evidence**:\n1. **CV AUC = 0.917** but **Holdout AUC = 0.667** — the model memorised training voices\n2. One dementia speaker's score **dropped** from 0.994 (CV) to 0.658 (holdout) — without training-set voice neighbours, prediction degrades\n3. One control speaker **jumped** from 0.318 (text-only holdout) to 0.992 (with HeAR) — HeAR encoded their unique voice as \"dementia-like\"\n4. **Dimensionality**: 4608 HeAR dims + 5120 MedGemma dims = 9728 features for 188 speakers (~52:1 ratio) — severely underdetermined, inviting overfitting\n5. **Alpha weights** in late fusion: HeAR received very low alpha (0.00-0.15), indicating the fusion model itself learnt to discount HeAR",
   "id": "cdb07d44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the HeAR vs text-only comparison on holdout\nprint(\"HeAR Impact on Individual Holdout Speakers\")\nprint(\"=\" * 80)\nprint(f\"{'Speaker':<18s} {'Group':<10s} {'Text+Acou':>10s} {'Text+HeAR':>10s} {'Delta':>8s} {'Interpretation'}\")\nprint(\"-\" * 80)\n\n# Text+acoustic narrative (no after symptoms)\ntan_scores = {s['speaker_name']: s['prob'] for s in holdout_tan['models']['text_acoustic_narrative_no_after_symptoms']['speakers']}\n# Feature concat (full model)\nhear_scores = {s['speaker_name']: s['prob'] for s in holdout_hear['models']['full_model']['speakers']}\n\n# Use real names for lookup, anonymous names for display\nHOLDOUT_NAMES = ['Bruce Willis', 'Gene Wilder', 'Tippi Hedren', 'Carol Burnett', 'Jane Goodall', 'Michael Caine']\nDEMENTIA_SET = {'Bruce Willis', 'Gene Wilder', 'Tippi Hedren'}\n\nfor name in HOLDOUT_NAMES:\n    tan_p = tan_scores.get(name, None)\n    hear_p = hear_scores.get(name, None)\n    if tan_p is not None and hear_p is not None:\n        group = 'dementia' if name in DEMENTIA_SET else 'control'\n        delta = hear_p - tan_p\n        interp = ''\n        if group == 'control' and delta > 0.1:\n            interp = 'HeAR WORSENED (false positive)'\n        elif group == 'dementia' and delta < -0.1:\n            interp = 'HeAR DEGRADED signal'\n        elif abs(delta) < 0.05:\n            interp = 'Minimal change'\n        else:\n            interp = 'HeAR pushed toward dementia'\n        print(f\"  {anon(name):<16s} {group:<10s} {tan_p:10.4f} {hear_p:10.4f} {delta:+8.4f}   {interp}\")",
   "id": "34fec382"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Confound Analysis\n",
    "\n",
    "Systematic testing of potential data leakage and confounding artefacts. Each test trains a classifier on **metadata or artefact features only** to see whether non-speech information can predict dementia status.\n",
    "\n",
    "### 4.1 Metadata Confound (Clip Count + Audio Duration)"
   ],
   "id": "f0192d8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metadata Confound Classifier — Clip Count + Audio Duration\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Features: clip_count, mean_duration, std_duration, total_duration\")\n",
    "print(\"Classifier: LogisticRegression (C=1.0, balanced, saga, L2)\")\n",
    "print(\"CV: StratifiedGroupKFold, 10 seeds x 5 folds\")\n",
    "print()\n",
    "\n",
    "metadata_results = [\n",
    "    {\n",
    "        \"Dataset\": \"Agentic (pre-symptoms)\",\n",
    "        \"N speakers\": 188,\n",
    "        \"Confound AUC\": 0.687,\n",
    "        \"Confound Std\": 0.018,\n",
    "        \"Real Model AUC\": 0.911,\n",
    "        \"Gap\": \"+0.224\",\n",
    "        \"Permutation p\": 0.000,\n",
    "    },\n",
    "    {\n",
    "        \"Dataset\": \"DementiaNet (pre-symptoms)\",\n",
    "        \"N speakers\": 182,\n",
    "        \"Confound AUC\": 0.806,\n",
    "        \"Confound Std\": None,\n",
    "        \"Real Model AUC\": 0.794,\n",
    "        \"Gap\": \"-0.012\",\n",
    "        \"Permutation p\": 0.000,\n",
    "    },\n",
    "]\n",
    "\n",
    "df_meta = pd.DataFrame(metadata_results)\n",
    "print(df_meta.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  AGENTIC: Metadata explains only part of the signal (AUC 0.687 vs model 0.911).\")\n",
    "print(\"    Gap of +0.224 demonstrates genuine speech content drives the real model.\")\n",
    "print(\"  DEMENTIANET: Metadata AUC (0.806) EXCEEDS the real model (0.794).\")\n",
    "print(\"    However, the real model never sees duration or clip count directly.\")\n",
    "print(\"    The critical question is: does the model ACCESS this information?\")\n",
    "print(\"    (See Section 4.2 for model-accessible confound analysis.)\")"
   ],
   "id": "ad5ee734"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model-Accessible Confound (Zero-Std Pattern)\n",
    "\n",
    "The classifier receives a `[mean(2560), std(2560)]` = 5120-dim feature vector per speaker.  \n",
    "For **single-clip speakers**, the std component is **identically zero** (2560 zeros).  \n",
    "\n",
    "This test checks whether the zero-std pattern alone can predict dementia — the **only information pathway** through which clip count can leak into the model."
   ],
   "id": "4dea9743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model-Accessible Confound — Zero-Std Pattern Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Features tested (all derivable from the actual embedding vector):\")\n",
    "print(\"  1. is_single_clip (binary: 1 if std component is all zeros)\")\n",
    "print(\"  2. n_zero_std_dims (how many of 2560 std dims are exactly 0)\")\n",
    "print(\"  3. std_norm (L2 norm of the std half — 0 for single-clip)\")\n",
    "print(\"  4. std_mean_abs (mean absolute value of std half)\")\n",
    "print()\n",
    "\n",
    "accessible_results = [\n",
    "    {\n",
    "        \"Dataset\": \"Agentic\",\n",
    "        \"Binary AUC\": 0.529,\n",
    "        \"Std-derived AUC\": 0.632,\n",
    "        \"All accessible AUC\": 0.632,\n",
    "        \"Real model AUC\": 0.901,\n",
    "        \"Gap\": \"+0.269\",\n",
    "    },\n",
    "    {\n",
    "        \"Dataset\": \"DementiaNet\",\n",
    "        \"Binary AUC\": 0.709,\n",
    "        \"Std-derived AUC\": 0.775,\n",
    "        \"All accessible AUC\": 0.775,\n",
    "        \"Real model AUC\": 0.794,\n",
    "        \"Gap\": \"+0.020\",\n",
    "    },\n",
    "]\n",
    "\n",
    "df_acc = pd.DataFrame(accessible_results)\n",
    "print(df_acc.to_string(index=False))\n",
    "\n",
    "print(\"\\nSingle-clip speaker distribution:\")\n",
    "print(\"  AGENTIC:\")\n",
    "print(\"    Dementia:  5/84  (6.0%) are single-clip\")\n",
    "print(\"    Control:   0/104 (0.0%) are single-clip\")\n",
    "print(\"    => Minimal imbalance; binary feature near chance (AUC 0.529)\")\n",
    "print(\"  DEMENTIANET:\")\n",
    "print(\"    Dementia: 47/74  (63.5%) are single-clip\")\n",
    "print(\"    Control:  21/98  (21.4%) are single-clip\")\n",
    "print(\"    => SUBSTANTIAL imbalance; binary feature alone AUC 0.709\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  AGENTIC: +0.269 gap confirms genuine speech content signal.\")\n",
    "print(\"  DEMENTIANET: +0.020 gap is CONCERNING — the zero-std artefact nearly\")\n",
    "print(\"    explains the full model performance. This is because the human-curated\")\n",
    "print(\"    dataset has many dementia subjects with only a single available video.\")"
   ],
   "id": "79acd6b8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 StratifiedGroupKFold Stability Analysis\n",
    "\n",
    "GroupKFold produces deterministic splits that may not balance class proportions across folds.  \n",
    "StratifiedGroupKFold balances class ratios but introduces seed-dependent variance."
   ],
   "id": "45bc754f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"StratifiedGroupKFold vs GroupKFold — Real Model\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Dataset: Agentic pre-symptoms (188 speakers, 2571 clips)\")\n",
    "print(\"Model: text + acoustic narrative, LogReg (C=1.0, balanced, saga, L2)\")\n",
    "print()\n",
    "print(f\"{'CV Strategy':<30s} {'Mean AUC':>10s} {'Std':>8s} {'Range':>20s}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  {'GroupKFold (deterministic)':<28s} {'0.9112':>10s} {'0.0420':>8s} {'0.8655 - 0.9697':>20s}\")\n",
    "print(f\"  {'StratifiedGKF (10 seeds)':<28s} {'0.9006':>10s} {'0.0079':>8s} {'0.8869 - 0.9101':>20s}\")\n",
    "print()\n",
    "print(\"Observation:\")\n",
    "print(\"  StratifiedGroupKFold gives slightly lower mean AUC (0.901 vs 0.911)\")\n",
    "print(\"  but with MUCH lower variance (0.008 vs 0.042).\")\n",
    "print(\"  This suggests GroupKFold's higher AUC is partly due to a lucky split.\")\n",
    "print(\"  The true expected performance is likely ~0.90 AUC.\")"
   ],
   "id": "ab3c88e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ROCKET Clustering — Recording Quality Confound\n",
    "\n",
    "MiniRocket features on full mel spectrograms can cluster recordings by **era and quality** (older recordings vs newer ones).  \n",
    "The frequency band ablation (Section 1.4) showed the high-frequency band (4+ kHz) carries the **least diagnostic signal**, confirming that recording quality differences are not the primary driver.\n",
    "\n",
    "### 4.5 Post-Symptom Exclusion Impact"
   ],
   "id": "f93b7bfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Post-Symptom Exclusion Impact\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"{'Model':<50s} {'All Data':>10s} {'Pre-Symp':>10s} {'Delta':>8s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "comparisons = [\n",
    "    (\"Text + acoustic narrative (LogReg)\", 0.9077, 0.9016, -0.0061),\n",
    "    (\"Text + acoustic narrative (CV mean)\", 0.9211, 0.9112, -0.0099),\n",
    "    (\"Late fusion (text + HeAR)\", 0.9127, 0.9047, -0.0080),\n",
    "    (\"Feature concat (text + HeAR)\", 0.9203, 0.9069, -0.0134),\n",
    "    (\"HeAR standalone\", 0.7258, 0.7573, +0.0315),\n",
    "]\n",
    "\n",
    "for name, all_auc, pre_auc, delta in comparisons:\n",
    "    print(f\"  {name:<48s} {all_auc:10.4f} {pre_auc:10.4f} {delta:+8.4f}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  Excluding post-symptom clips drops AUC by ~0.006-0.013 for text-based models.\")\n",
    "print(\"  This modest drop shows the model is NOT primarily relying on obvious\")\n",
    "print(\"  post-diagnosis speech deterioration — it detects PRE-symptomatic markers.\")\n",
    "print(\"  HeAR actually IMPROVES slightly (+0.032), suggesting post-symptom clips\")\n",
    "print(\"  were adding voice-fingerprint noise to the HeAR features.\")"
   ],
   "id": "f3440bf4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Additional Controls\n",
    "\n",
    "**Random embeddings baseline**: Replacing MedGemma embeddings with random vectors yields ~0.50 AUC (chance), confirming the model depends on actual embedding content.\n",
    "\n",
    "**Vision-only results**: MedGemma image-only achieves 0.636 AUC (agentic) and 0.702 AUC (DementiaNet) — well below text-based models, ruling out spectrogram artefacts as the primary signal source.\n",
    "\n",
    "**Acoustic variability alone**: Training on acoustic variability features (speech rate, pause ratio, pitch statistics) yields AUC = 0.518 — essentially chance. Adding acoustic variability to text embeddings slightly **degrades** performance (0.900 vs 0.904). The acoustic metrics are useful only when embedded as natural language in the text prompt (acoustic narrative)."
   ],
   "id": "402b5bc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Acoustic variability results\nav = load_json(BASE / \"acoustic_variability_results.json\")\n\nprint(\"Acoustic Variability Analysis\")\nprint(\"=\" * 70)\nav_emb = av['acoustic_variability_card_text_embedding']\nfus = av['fusion_with_saved_text_anchor']\n\nprint(f\"  Acoustic variability embeddings alone:     AUC = {av_emb['mean_auc']:.4f} +/- {av_emb['std_auc']:.4f}\")\nprint(f\"  Text-only baseline (Full LogReg):          AUC = {fus['base_saved_text_full_logreg']['mean_auc']:.4f}\")\nprint(f\"  Fused text + acoustic var (Full LogReg):   AUC = {fus['fused_saved_text_plus_acoustic_card_full_logreg']['mean_auc']:.4f}\")\nprint(f\"  Text-only baseline (PCA-128):              AUC = {fus['base_saved_text_pca128_logreg']['mean_auc']:.4f}\")\nprint(f\"  Fused text + acoustic var (PCA-128):       AUC = {fus['fused_saved_text_plus_acoustic_card_pca128_logreg']['mean_auc']:.4f}\")\nprint()\nprint(\"Conclusion: Acoustic variability features carry NO independent diagnostic signal (0.518 AUC).\")\nprint(\"Fusing them with text embeddings slightly DEGRADES performance (0.896 vs 0.904).\")\nprint(\"They are useful only when converted to natural language and embedded via MedGemma.\")",
   "id": "0e2732df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Anti-Leakage Image Analysis"
   ],
   "id": "12853b22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Anti-leakage image results\nanti_leak = load_json(BASE / \"anti_leakage_image_results.json\")\n\nprint(\"Anti-Leakage Image Representations\")\nprint(\"=\" * 70)\nprint(\"Testing whether alternative spectrogram representations leak diagnosis info:\")\nprint()\nfor key, val in anti_leak.items():\n    print(f\"  {key:<25s}: AUC = {val:.3f}\")\n\nprint(\"\\nAll near chance (0.50) — no image-based data leakage detected.\")",
   "id": "26ffbd3b"
  },
  {
   "cell_type": "markdown",
   "source": "### 4.8 Dimensionality Robustness Analysis\n\nWith 5120 features (mean+std) and only 188 speakers (Agentic) or 167 speakers (DementiaNet), the feature-to-sample ratio is 27:1 and 31:1 respectively — well into territory where overfitting is a concern.\n\nSix experiments probe whether performance is driven by genuine signal or high-dimensional artefacts:\n\n| Experiment | Question | Method |\n|-----------|----------|--------|\n| 1. Mean-only ablation | Does the std half add genuine signal? | Compare 2560 vs 5120 dims |\n| 2. PCA sweep | Does PCA-16 retain most of the signal? | PCA from 16 to 5120 dims |\n| 3. C sweep | Is performance sensitive to regularisation? | C from 0.001 to 100 |\n| 4. Learning curve | Does AUC plateau or keep climbing? | Subsample speakers 20%-100% |\n| 5. Permutation test | Is the model significantly above chance? | 200 label shuffles |\n| 6. Mean-only PCA sweep | Can we compress mean-only further? | PCA from 16 to 2560 dims |",
   "metadata": {},
   "id": "b87fb33b"
  },
  {
   "cell_type": "code",
   "source": "# Load dimensionality robustness results\ndim_results = load_json(NOTEBOOKS / \"dimensionality_robustness_results.json\")\n\n# ── Experiment 1: Mean-only vs Mean+Std ──\nprint(\"EXPERIMENT 1: Mean-only (2560) vs Mean+Std (5120) Ablation\")\nprint(\"=\" * 90)\nprint(f\"  {'Dataset':<15s} {'Mean+Std':>10s} {'Mean-only':>11s} {'Delta':>8s} {'M+S PCA128':>12s} {'M PCA128':>10s} {'PCA Delta':>11s}\")\nprint(\"-\" * 80)\nfor name in [\"Agentic\", \"DementiaNet\"]:\n    r = dim_results[\"mean_vs_mean_std\"][name]\n    ms = r[\"mean_std_full\"][\"auc\"]\n    m = r[\"mean_only_full\"][\"auc\"]\n    ms_p = r[\"mean_std_pca128\"][\"auc\"]\n    m_p = r[\"mean_only_pca128\"][\"auc\"]\n    print(f\"  {name:<15s} {ms:10.4f} {m:11.4f} {ms-m:+8.4f} {ms_p:12.4f} {m_p:10.4f} {ms_p-m_p:+11.4f}\")\nprint()\nprint(\"Interpretation: Std half adds +0.048 AUC on Agentic (genuine temporal variability signal)\")\nprint(\"  and +0.019 on DementiaNet. The delta persists under PCA-128, confirming it is not\")\nprint(\"  a dimensionality artefact. The std component captures meaningful inter-clip variance.\")\n\n# ── Experiment 2: PCA sweep ──\nprint(f\"\\n\\nEXPERIMENT 2: PCA Sweep (Mean+Std, 5120 dims)\")\nprint(\"=\" * 90)\nprint(f\"  {'Dims':<12s} {'Ratio':>8s}  {'Agentic':>9s} {'DementiaNet':>13s}\")\nprint(\"-\" * 50)\nfor label in [\"PCA-16\", \"PCA-32\", \"PCA-64\", \"PCA-128\", \"PCA-256\", \"PCA-512\", \"Full-5120\"]:\n    ag = dim_results[\"pca_sweep\"][\"Agentic\"].get(label, {})\n    dn = dim_results[\"pca_sweep\"][\"DementiaNet\"].get(label, {})\n    ratio = f\"{ag.get('ratio', '—')}:1\" if ag else \"—\"\n    ag_auc = ag.get(\"auc\", float(\"nan\"))\n    dn_auc = dn.get(\"auc\", float(\"nan\"))\n    print(f\"  {label:<12s} {ratio:>8s}  {ag_auc:9.4f} {dn_auc:13.4f}\")\nprint()\nprint(\"Key finding: PCA-16 retains 0.896 AUC on Agentic (vs 0.910 full) — only -0.014 drop.\")\nprint(\"  With a 0.1:1 dim:speaker ratio, this rules out high-dimensional overfitting.\")\nprint(\"  DementiaNet shows similar robustness: PCA-32 = 0.811 vs Full = 0.839.\")\n\n# ── Experiment 3: C sweep ──\nprint(f\"\\n\\nEXPERIMENT 3: Regularisation C Sweep\")\nprint(\"=\" * 90)\nprint(f\"  {'C':<10s} {'Agentic':>10s} {'DementiaNet':>13s}\")\nprint(\"-\" * 40)\nfor c_key in [\"C=0.001\", \"C=0.01\", \"C=0.1\", \"C=0.5\", \"C=1.0\", \"C=5.0\", \"C=10.0\", \"C=100.0\"]:\n    ag = dim_results[\"c_sweep\"][\"Agentic\"].get(c_key, {}).get(\"auc\", float(\"nan\"))\n    dn = dim_results[\"c_sweep\"][\"DementiaNet\"].get(c_key, {}).get(\"auc\", float(\"nan\"))\n    print(f\"  {c_key:<10s} {ag:10.4f} {dn:13.4f}\")\nprint()\nprint(\"Agentic: Nearly flat (0.907-0.911) across 5 orders of magnitude — L2 regularisation\")\nprint(\"  barely affects performance. This is inconsistent with overfitting (an overfit model\")\nprint(\"  would degrade sharply at high C / weak regularisation).\")\nprint(\"DementiaNet: Also stable (0.828-0.841), with slight preference for moderate C (0.1).\")\n\n# ── Experiment 4: Learning curve ──\nprint(f\"\\n\\nEXPERIMENT 4: Learning Curve (Subsample Speakers)\")\nprint(\"=\" * 90)\nfor name in [\"Agentic\", \"DementiaNet\"]:\n    print(f\"\\n  {name}:\")\n    curve = dim_results[\"learning_curve\"][name]\n    print(f\"    {'Fraction':<10s} {'Speakers':>10s} {'AUC':>8s} {'Std':>8s}\")\n    print(\"    \" + \"-\" * 40)\n    for frac_key in sorted(curve.keys(), key=float):\n        v = curve[frac_key]\n        auc_val = v[\"mean_auc\"]\n        if np.isnan(auc_val):\n            continue\n        print(f\"    {float(frac_key):<10.0%} {v['n_speakers']:10d} {auc_val:8.4f} {v['std_auc']:8.4f}\")\nprint()\nprint(\"Agentic: Plateaus at ~50% of speakers (0.901 AUC), consistent with genuine signal.\")\nprint(\"  An overfit model would show steep improvement all the way to 100%.\")\nprint(\"DementiaNet: Slower climb, still rising at 100% — would benefit from more speakers.\")\n\n# ── Experiment 5: Permutation test ──\nprint(f\"\\n\\nEXPERIMENT 5: Permutation Test (200 shuffles)\")\nprint(\"=\" * 90)\nprint(f\"  {'Dataset':<15s} {'Real AUC':>10s} {'Null Mean':>11s} {'Null Std':>10s} {'95th Pct':>10s} {'p-value':>9s} {'Effect':>9s}\")\nprint(\"-\" * 80)\nfor name in [\"Agentic\", \"DementiaNet\"]:\n    r = dim_results[\"permutation_test\"][name]\n    print(f\"  {name:<15s} {r['real_auc']:10.4f} {r['null_mean']:11.4f} {r['null_std']:10.4f} \"\n          f\"{r['null_95th']:10.4f} {r['p_value']:9.4f} {r['effect_size']:+9.4f}\")\nprint()\nprint(\"Both datasets are HIGHLY SIGNIFICANT (p = 0.000 on 200 permutations).\")\nprint(\"  Agentic effect size: +0.416 (real 0.910 vs null 0.493)\")\nprint(\"  DementiaNet effect size: +0.342 (real 0.839 vs null 0.497)\")\nprint(\"  Even the null 95th percentile (0.587 / 0.592) is far below real model AUC.\")\nprint(\"  This definitively rules out chance-level performance from overfitting.\")\n\n# ── Experiment 6: PCA sweep on mean-only ──\nprint(f\"\\n\\nEXPERIMENT 6: PCA Sweep on Mean-Only (2560 dims)\")\nprint(\"=\" * 90)\nprint(f\"  {'Dims':<12s} {'Ratio':>8s}  {'Agentic':>9s} {'DementiaNet':>13s}\")\nprint(\"-\" * 50)\nfor label in [\"PCA-16\", \"PCA-32\", \"PCA-64\", \"PCA-128\", \"PCA-256\", \"Full-2560\"]:\n    ag = dim_results[\"pca_sweep_mean_only\"][\"Agentic\"].get(label, {})\n    dn = dim_results[\"pca_sweep_mean_only\"][\"DementiaNet\"].get(label, {})\n    ratio = f\"{ag.get('ratio', '—')}:1\" if ag else \"—\"\n    ag_auc = ag.get(\"auc\", float(\"nan\"))\n    dn_auc = dn.get(\"auc\", float(\"nan\"))\n    print(f\"  {label:<12s} {ratio:>8s}  {ag_auc:9.4f} {dn_auc:13.4f}\")\nprint()\nprint(\"Mean-only PCA-16 still achieves 0.853 on Agentic (vs 0.861 full) — the core signal\")\nprint(\"  is captured in very few principal components.\")\n\n# ── Overall summary ──\nprint(f\"\\n\\n{'='*90}\")\nprint(\"DIMENSIONALITY ROBUSTNESS — OVERALL VERDICT\")\nprint(f\"{'='*90}\")\nprint()\nprint(\"AGENTIC DATASET: Performance is ROBUST against dimensionality concerns.\")\nprint(\"  - PCA-16 retains 98.5% of full AUC (0.896 vs 0.910)\")\nprint(\"  - C sweep flat across 5 orders of magnitude\")\nprint(\"  - Learning curve plateaus at 50% of speakers\")\nprint(\"  - Permutation test p < 0.005 with +0.416 effect size\")\nprint(\"  - Std half adds genuine +0.048 AUC\")\nprint()\nprint(\"DEMENTIANET DATASET: Performance is LARGELY ROBUST but with caveats.\")\nprint(\"  - PCA-32 retains 96.6% of full AUC (0.811 vs 0.839)\")\nprint(\"  - Learning curve still rising — would benefit from more speakers\")\nprint(\"  - Permutation test p < 0.005 with +0.342 effect size\")\nprint(\"  - Combined with the single-clip confound (Section 4.2), ~0.775 of the 0.839 AUC\")\nprint(\"    may be partially attributable to metadata structure\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "10c2205f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comprehensive Results Summary\n",
    "\n",
    "### 5.1 Final Comparison Table"
   ],
   "id": "4007998e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"COMPREHENSIVE RESULTS SUMMARY\")\nprint(\"=\" * 110)\nprint()\n\n# DementiaNet section — now with pre-symptoms comparison\nprint(\"DementiaNet (human-curated)\")\nprint(\"-\" * 100)\ndnet_summary = [\n    #  (Model, Classifier, All AUC, All F1, All Acc, Pre AUC, Pre F1, Pre Acc)\n    (\"HeAR frozen embeddings\",          \"SVM-RBF\",        0.748, 0.632, 0.681, 0.773, 0.675, 0.730),\n    (\"HeAR frozen embeddings\",          \"Full LogReg\",    0.729, None,  None,  0.791, 0.667, 0.719),\n    (\"MiniRocket (model-free control)\", \"PCA-128 LogReg\", 0.770, 0.680, 0.731, 0.743, 0.649, 0.706),\n    (\"MedGemma text (Pyannote)\",        \"Full LogReg\",    0.792, None,  None,  0.838, 0.780, 0.802),\n    (\"MedGemma text (Pyannote)\",        \"Full GBM\",       0.794, 0.683, 0.706, 0.800, 0.691, 0.736),\n    (\"MedGemma vision (image-only)\",    \"Full GBM\",       0.730, 0.609, 0.653, 0.804, 0.708, 0.757),\n    (\"MedGemma image+text\",             \"Full GBM\",       0.788, 0.700, 0.742, 0.815, 0.681, 0.755),\n    (\"Late fusion (HeAR+text stack)\",   \"Stacking\",       0.793, 0.701, 0.739, None,  None,  None),\n]\n\nprint(f\"  {'Model':<36s} {'Classifier':<16s} {'All AUC':>9s} {'Pre AUC':>9s} {'Delta':>8s} {'Pre F1':>8s} {'Pre Acc':>9s}\")\nfor name, clf, all_auc, all_f1, all_acc, pre_auc, pre_f1, pre_acc in dnet_summary:\n    if pre_auc is not None:\n        delta = pre_auc - all_auc\n        pre_f1_s = f\"{pre_f1:8.3f}\" if pre_f1 is not None else \"      — \"\n        pre_acc_s = f\"{pre_acc:9.3f}\" if pre_acc is not None else \"       — \"\n        print(f\"  {name:<36s} {clf:<16s} {all_auc:9.3f} {pre_auc:9.3f} {delta:+8.3f} {pre_f1_s} {pre_acc_s}\")\n    else:\n        print(f\"  {name:<36s} {clf:<16s} {all_auc:9.3f} {'—':>9s} {'—':>8s} {'—':>8s} {'—':>9s}\")\n\nprint(f\"\\n  Note: 'All' = 182 speakers, 354 clips (incl. after-symptoms)\")\nprint(f\"        'Pre' = 167 speakers, ~315 clips (after-symptoms excluded)\")\nprint(f\"  Most models IMPROVE with pre-symptoms only filtering (+0.02 to +0.07).\")\n\n# Agentic section\nprint(f\"\\nLLM-Assisted / Agentic (196 speakers, ~3,100 clips)\")\nprint(\"-\" * 100)\nag_summary = [\n    (\"MedASR transcripts (chunked)\",     \"—\",                0.619, None,  None),\n    (\"Manifest transcripts (ablation)\",  \"LogReg\",           0.895, 0.813, 0.827),\n    (\"MedGemma text-only (final)\",       \"LogReg\",           0.904, 0.802, 0.811),\n    (\"MedGemma vision (image-only)\",     \"LogReg PCA-64\",    0.636, 0.491, 0.556),\n    (\"Feature concat (text+HeAR)\",       \"LogReg\",           0.917, 0.862, 0.878),\n    (\"Acoustic narrative (all data)\",    \"LogReg\",           0.921, 0.809, 0.816),\n    (\"Acoustic narrative (pre-symp) ★\",  \"LogReg\",           0.911, 0.829, 0.851),\n]\n\nprint(f\"  {'Model':<38s} {'Classifier':<18s} {'AUC':>7s} {'F1':>7s} {'Acc':>7s}\")\nfor name, clf, auc, f1, acc in ag_summary:\n    f1_s = f\"{f1:7.3f}\" if f1 is not None else \"    —  \"\n    acc_s = f\"{acc:7.3f}\" if acc is not None else \"    —  \"\n    print(f\"  {name:<38s} {clf:<18s} {auc:7.3f} {f1_s} {acc_s}\")\n\n# Holdout section — dynamically computed after excluding force-analysed speakers\nprint(f\"\\nHoldout Evaluation (6 celebrity speakers, fully unseen)\")\nprint(\"-\" * 100)\nholdout_models = [\n    (\"Feature concat (text+HeAR)\", filter_holdout_model(holdout_hear['models']['full_model'])),\n    (\"Text+acoustic narrative (pre-symp)\", filter_holdout_model(holdout_tan['models']['text_acoustic_narrative_no_after_symptoms'])),\n    (\"Text+acoustic narrative (all data)\", filter_holdout_model(holdout_tan['models']['text_acoustic_narrative'])),\n    (\"Text-only\", filter_holdout_model(holdout_text['results'])),\n]\n\nprint(f\"  {'Model':<38s} {'AUC':>7s} {'Acc':>7s} {'Notes'}\")\nfor name, fm in holdout_models:\n    notes = f\"{fm['n_correct']}/{fm['n_speakers']} correct\"\n    if 'HeAR' in name:\n        notes += \"; FAILED: voice fingerprinting\"\n    print(f\"  {name:<38s} {fm['auc']:7.3f} {fm['accuracy']:7.3f}   {notes}\")\n\n# Confound section\nprint(f\"\\nConfound Checks\")\nprint(\"-\" * 100)\nconfound_summary = [\n    (\"Metadata (clip count + duration)\",  \"Agentic\",     0.687, \"Mild; gap +0.224 to real model\"),\n    (\"Metadata (clip count + duration)\",  \"DementiaNet\", 0.806, \"EXCEEDS real model (0.794)\"),\n    (\"Model-accessible (zero-std)\",       \"Agentic\",     0.632, \"Clean gap +0.269\"),\n    (\"Model-accessible (zero-std)\",       \"DementiaNet\", 0.775, \"Concerning gap +0.020\"),\n    (\"Acoustic variability alone\",        \"Agentic\",     0.518, \"Essentially chance\"),\n    (\"Random embeddings\",                 \"—\",           0.500, \"Chance (control)\"),\n    (\"Image anti-leakage\",               \"Agentic\",     0.576, \"Near chance\"),\n]\n\nprint(f\"  {'Confound Test':<38s} {'Dataset':<14s} {'AUC':>7s} {'Interpretation'}\")\nfor name, dataset, auc, interp in confound_summary:\n    print(f\"  {name:<38s} {dataset:<14s} {auc:7.3f}   {interp}\")",
   "id": "abec603f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Key Conclusions\n\n1. **Text content is the dominant signal**: MedGemma text embeddings consistently outperform all audio-only approaches (HeAR, ROCKET, vision encoder) on both datasets.\n\n2. **Acoustic narrative prompting is effective**: Enriching the text prompt with waveform-derived acoustic metrics (speech rate, pause ratio, pitch) improves AUC from 0.904 to 0.921 (all data) / 0.911 (pre-symptoms) — without adding model parameters.\n\n3. **HeAR features overfit to speaker identity**: CV AUC 0.917 collapsed to holdout AUC 0.667. The 4608-dim CLS attention features encode voice fingerprints, not cognitive markers. HeAR was dropped from the final model.\n\n4. **The agentic dataset has genuine speech signal**: +0.269 gap between model-accessible confound (0.632) and real model (0.901) confirms the model learns from speech content, not metadata artefacts.\n\n5. **DementiaNet has a single-clip confound**: 63.5% of dementia speakers have only one clip (vs 21.4% controls). The zero-std pattern nearly explains the full model AUC (0.775 vs 0.794). Results on DementiaNet should be interpreted with this caveat.\n\n6. **Post-symptom exclusion has minimal impact**: Dropping after-symptoms clips reduces AUC by only ~0.01, demonstrating the model captures **pre-symptomatic** markers.\n\n7. **MiniRocket sets a useful baseline**: At 0.770 AUC on DementiaNet, random convolutional kernels on mel spectrograms provide a strong model-free reference point. Any learned representation must justify its complexity by exceeding this.\n\n8. **Recording quality is not the driver**: ROCKET frequency ablation shows signal concentrates in prosodic (0–1 kHz) and articulatory (1–4 kHz) bands, not high-frequency recording artefacts.\n\n9. **DementiaNet pre-symptoms re-run strengthens results**: Excluding 27 after-symptoms clips (10 speakers) **improved** most models — MedGemma text rose from 0.794 to 0.838, vision from 0.730 to 0.804, image+text from 0.788 to 0.815. This suggests after-symptoms clips were adding noise rather than signal, and the models genuinely detect pre-symptomatic markers even in the smaller DementiaNet dataset.\n\n10. **Dimensionality robustness confirmed**: Despite a 27:1 feature-to-sample ratio, the model is NOT overfitting. PCA-16 retains 98.5% of full AUC on Agentic; C sweep is flat across 5 orders of magnitude; learning curve plateaus at 50% of speakers; and permutation tests yield p < 0.005 with effect sizes of +0.416 (Agentic) and +0.342 (DementiaNet). The signal resides in a low-dimensional subspace of the MedGemma embedding manifold — the L2-regularised logistic regression effectively ignores the vast majority of input dimensions.",
   "id": "2e9dd41f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n*Notebook created as part of the SpeechSense cognitive decline monitoring project.*  \n*All cross-validation uses speaker-grouped folds to prevent data leakage.*  \n*Holdout evaluation uses fully unseen speakers with no model selection or hyperparameter tuning.*",
   "id": "275b39a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}